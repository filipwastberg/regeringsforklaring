---
title: "Analys av regeringsförklaringarna"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

Data kommer från en csv-fil där varje regeringsförklaring är uppdelad efter meningar eller stycken. Det finns också en kolumn för statsminister och en för datum för regeringsförklaringen.

```{r}
library(tidyverse)
regf <- read_csv("regf.csv")

glimpse(regf)
```

För att analysera regeringsförklaringarna så kommer jag att använda mig av paketet `tidytext` av Julia Silge och David Robinson. Det är ett av många paket som finns tillgängliga för att göra textanalys i R. Vad som gör att det här sticker ut är att det följer principer om *tidy data* vilket innebär att man alltid strävar efter att ha observationer i rader och variabler i kolumner vilket för visualisering och modellering av data.

Till att börja med gör jag om textvariabeln så att varje ord blir en rad. Det här gör jag med funktionen `unnest_tokens`.
```{r}
library(tidytext)
library(ggplot2)
tidy_regf <- regf %>%
  filter(!str_detect(text, "Regeringsförklaring")) %>%
  unnest_tokens(ord, text)

tidy_regf
```

Resultatet blir 130 000 ord. 

Nästa steg är att rensa data från stoppord såsom *och*, *på*, *skall* med mera. Det finns en del bibliotek för det här, exempelvis projektet `snowball` men jag kompletterar även med en lista som `Peter Dahlgren` forskare från Göteborg Universitet tagit fram. Den finns (här)[https://gist.github.com/peterdalle/8865eb918a824a475b7ac5561f2f88e9].

För enkelhetens skull har jag sparat den som en csv-fil.

```{r}
sv_stoppord <- read_csv("stoppord.csv")
tvättade_regf <- tidy_regf %>%
  filter(!str_detect(ord, "[[:digit:]]")) %>%
  anti_join(get_stopwords(language = "sv"), by = c("ord" = "word")) %>%
  anti_join(sv_stoppord, by = c("ord" = "stoppord"))

tvättade_regf
```

Genom att rensa för stoppord så halverade vi nästan antalet ord. Nu kan vi börja analysera! 

Den första frågan jag har är: vilka är de vanligast använda orden? 

```{r}
tvättade_regf %>%
  count(ord, sort = TRUE) %>%
  head(10) %>%
  ggplot(aes(x = reorder(ord, n), y = n, fill = ord)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  labs(title = "Vanligast ord i svenska regeringsförklaringar",
       x = "Ord", y = "Antal ord") +
  scale_fill_viridis_d(guide = FALSE)
```

Det är väl inte särskilt förvånande att Sverige toppar listan.

Följfrågan här blir hur det här varierar mellan olika statsministrar. Vem använder vilka ord mest?

```{r}
tvättade_regf %>%
  group_by(statsminister) %>%
  count(ord) %>%
  top_n(n = 10) %>%
  ggplot(aes(x = drlib::reorder_within(ord, n, statsminister), y = n)) +
  geom_col() +
  drlib::scale_x_reordered() +
  facet_wrap(~statsminister, scales = "free") +
  coord_flip() +
  labs(x = "", y = "Antal ord")
```

Det här är intressant men vi stöter här på ett problem som är vanligt i textanalys. Vissa ord nämns väldigt ofta men är inte nödvändigtvis viktiga för det. Att Reinfeldt nämner arbete, ansvar och jobb är inte förvånande och har ett visst analysvärde, men att Olof Palme nämner regeringen flest gånger har inget större analytiskt värde.

Om vi tittar på fördelnignen av ord så kommer vi se att den är väldigt skev. Det vill säga att vissa ord nämns väldigt ofta men de flesta gör inte det. 

```{r}
tvättade_regf %>%
  group_by(statsminister) %>%
  count(ord) %>%
  mutate(total = sum(n)) %>%
  ggplot(aes(n/total, fill = statsminister)) +
  geom_histogram(show.legend = FALSE) +
 # xlim(NA, 0.0009) +
  facet_wrap(~statsminister, scales = "free_y")
```

Den här typen av fördelning är vanlig i språk och textanalys. Jag kommer inte gå in på den mer här men för den som är nyfiken är den väl dokumenteterad i boken `Tidy Text Mining` som mycket av analyserna här baseras på. 

Ett sätt att hantera den här typen av problem är att väga upp de ord som inte används ofta och väga ner de som används ofta. Det kallas för `term frequency and inverse document frequency` och för det finns en funktion, `bind_tf_idf()`.

```{r}
tvättade_regf %>%
  group_by(statsminister) %>%
  count(ord) %>%
  bind_tf_idf(ord, statsminister, n) %>%
  top_n(n = 5) %>%
  ggplot(aes(x = drlib::reorder_within(ord, tf_idf, statsminister), y = tf_idf)) +
  geom_col() +
  drlib::scale_x_reordered() +
  facet_wrap(~statsminister, scales = "free") +
  coord_flip() +
  labs(x = "", y = "Antal ord")
```

Det här ger en något mer nyanserad bild av de olika regeringsförklaringarna. Vi ser dock att en del av regeringsförklaringarna lider av dåligt kvalité. Det beror på att de scannats in som ett steg i att digitalisera alla riksdagens dokument. Därmed är det många stoppord som inte har rensats, exempelvis har "att" blivit "atl" och "ett" blivit "elt" osv.

En annan aspekt är att Torbjörn Fälldin som hade relativt korta regeringsförklaringar får väldigt många ord uppvägda. Därför har jag dragit ner redovisning av antal ord till Top 5. 

## Antal ord över tid

En annan intressant faktor är hur långa respektiva regeringsförklaring var. Fram till mitten av 70-talet var det kungen som höll regeringsförklaringen. Hur den utvecklats över tid är därför intressant.

```{r}
tvättade_regf %>%
  group_by(datum) %>%
  count(datum) %>%
  ggplot(aes(x = datum, y = n)) +
  geom_line() +
  geom_smooth(method = "loess") +
  theme_minimal() +
  labs(title = "Antal ord per regeringsförklaring över tid",
       y = "Antal ord",
       x = "") +
  scale_y_continuous(limits = c(0,3500))
```

Det är tydligt att regeringsförklaringarna blivit längre. Samtidigt är det relativt stor variation över tid. Vilken statsminister hade längst regeringsförklaringar? Nedan ser vi medelvärde för antal ord per regeringsförklaring för varje statsminister. 

```{r}
tvättade_regf %>%
  group_by(statsminister, datum) %>%
  summarise(antal_ord = n()) %>%
  group_by(statsminister) %>%
  summarise(mv_ord = mean(antal_ord)) %>% 
  ggplot(aes(x = reorder(statsminister, mv_ord),
             y = mv_ord, fill = statsminister)) +
  geom_col() +
  coord_flip() +
  labs(y = "Medelvärde antal ord",
       x = "",
       title = "Medelvärde antal ord per regeringförklaring") +
  theme_minimal() +
  scale_fill_viridis_d() +
  guides(fill = FALSE)
```

Den sista analysen vi gör av regeringsförklaringarna handlar om relationen mellan ord. Det vi gör då är helt enkel att räkna antalet gånger olika ord nämns tillsammans. Kort sagt räknar vi två ord i taget istället för ett ord i taget. 

Med `tidytext` gör vi det genom att specificera hur många `ngrams` vi vill använda. Under motorhuven används här paketet `tokenizers`.

```{r}
bigram_regf <- regf %>%
  filter(!is.na(statsminister)) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
bigram_regf
```

Vi kan nu räkna vilka ord som nämns tillsammans mest. 

```{r}
bigram_regf %>%
   count(bigram, sort = TRUE)
```

Vi ser att stopporden är tillbaka. Det kan vi lösa genom att dela upp våra bigrams i två kolumner och sedan filtrera bort stopporden i respektive kolumn.

```{r}
library(tidyr)
bigrams_separated <- bigram_regf %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter_at(vars(word1, word2), all_vars(!. %in% sv_stoppord$stoppord))

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts
```

När vi nu har rensat för stoppord kan vi stå ihop orden till en kolumn.

```{r}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```
Vi kan åter igen göra en 

```{r}
bigram_tf_idf <- bigrams_united %>%
  count(statsminister, bigram) %>%
  bind_tf_idf(bigram, statsminister, n) %>%
  arrange(desc(tf_idf))
```

```{r}
library(igraph)
library(ggraph)
library(dplyr)
bigram_graph <- bigram_counts %>%
  filter(n > 15) %>%
  graph_from_data_frame()

set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```






